{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952,
     "referenced_widgets": [
      "55ae644005024afe9feeeb31760527a8",
      "95240ed468dc47998fae4dc1134af6a8",
      "954427ac4286440aa85ff8b04c3f8e25",
      "eafd48778a86454cbe41951af0f3c8fa",
      "0e7333d738114c43b9bf7a9b069b27c8",
      "8b232eda97694d3a834b7a19beee4add",
      "b48970b6b2db4f6f8c2c0e7fa1684c18",
      "7cf3390402204233893b280919c9577b",
      "b50288a8106c42efafe19a82b301f4f1",
      "d0110e24d2cb457eba581147819ad2e7",
      "31f108269d654f76bb1e0cc217fe3447",
      "22d3627ff8a841e7b4f148a91fff7136",
      "342f4d2272f743f7ace87995b6d261f3",
      "3cc4fcb3e0264dcdb8691e2150926c47",
      "0727e5d4a36e4cbebe80fe18fffb680a",
      "bcd0451b43c54a24a8f3f2fcffdf31ee",
      "9f58c6cb453a49a4811096438311e455",
      "6dd192358c914196b4599dbc4e410aba",
      "a4b534a1ef1941708c90a1ae51836650",
      "dc1f7d476d36447c97c1b25564ac6893",
      "e2e7eeb123f9404ea1779ffbb5e62798",
      "905e23c45f6c4219be92bff0202cc677"
     ]
    },
    "executionInfo": {
     "elapsed": 45332,
     "status": "ok",
     "timestamp": 1757733754827,
     "user": {
      "displayName": "Harish V",
      "userId": "12280508567512914485"
     },
     "user_tz": -330
    },
    "id": "RexNGiuXEu0e",
    "outputId": "fc1a2c06-5d2b-4752-fbc5-647a78097374"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (0.35.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers) (8.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from diffusers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from diffusers) (0.34.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from diffusers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.32.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from diffusers) (0.6.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from diffusers) (11.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.34.0->diffusers) (1.1.9)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2025.8.3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ae644005024afe9feeeb31760527a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The TextToVideoSDPipeline has been deprecated and will not receive bug fixes or feature updates after Diffusers version 0.33.1. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d3627ff8a841e7b4f148a91fff7136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved as output_video.mp4\n"
     ]
    }
   ],
   "source": [
    "!pip install diffusers transformers accelerate torch opencv-python\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
    "\n",
    "# Load the pipeline\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"damo-vilab/text-to-video-ms-1.7b\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\"\n",
    ")\n",
    "\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "# Generate video frames\n",
    "prompt = \"Spiderman is surfing\"\n",
    "result = pipe(prompt, num_inference_steps=25)\n",
    "video_frames_batches = result.frames  # This is a list of batches\n",
    "\n",
    "# Flatten all frames from batches into a single list\n",
    "all_frames = []\n",
    "for batch in video_frames_batches:\n",
    "    # batch shape: (16, 256, 256, 3)\n",
    "    for frame in batch:\n",
    "        all_frames.append(frame)\n",
    "\n",
    "# Check frame format and convert if needed\n",
    "corrected_frames = []\n",
    "for frame in all_frames:\n",
    "    if isinstance(frame, torch.Tensor):\n",
    "        frame = frame.cpu().numpy()\n",
    "    if frame.ndim == 2:  # Grayscale, expand to 3 channels\n",
    "        frame = np.stack([frame]*3, axis=-1)\n",
    "    elif frame.ndim == 3 and frame.shape[2] == 1:  # Single channel\n",
    "        frame = np.concatenate([frame]*3, axis=2)\n",
    "    elif frame.ndim == 3 and frame.shape[2] in [3, 4]:\n",
    "        pass  # Already correct format\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported frame shape: {frame.shape}\")\n",
    "    # Convert from float [0,1] to uint8 if needed\n",
    "    if frame.dtype != np.uint8:\n",
    "        frame = (frame * 255).clip(0, 255).astype(np.uint8)\n",
    "    corrected_frames.append(frame)\n",
    "\n",
    "# Define video parameters\n",
    "height, width, layers = corrected_frames[0].shape\n",
    "video_filename = 'output_video.mp4'\n",
    "fps = 20\n",
    "\n",
    "# Create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "video_writer = cv2.VideoWriter(video_filename, fourcc, fps, (width, height))\n",
    "\n",
    "# Write frames\n",
    "for frame in corrected_frames:\n",
    "    if frame.shape[2] == 4:  # RGBA -> RGB\n",
    "        frame = frame[:, :, :3]\n",
    "    video_writer.write(frame)\n",
    "\n",
    "video_writer.release()\n",
    "\n",
    "print(f\"Video saved as {video_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292,
     "referenced_widgets": [
      "2521b6ecb26442818beddd57f2540b4f",
      "c33538e0cfe14874ac61341e74b91363",
      "a0b46cd819a4467cac1ff4d909571683",
      "8bdb784662cf44769d1a038d721b18d5",
      "6424b22984684a819aa37546520f20cb",
      "c04c9450d6624fe2a4acc1f9f52848f8",
      "50e9778a11774becb26aa294573ed31a",
      "afda1b819cb840f09a2353aa5d6aee92",
      "9f4f157258af4b928c8e687ec0445fc3",
      "250add0996bf46e2b981d3ae8298b711",
      "9dae006941644eceae365ba54ceccc69",
      "e2ab60bec7c7488fa90d290db839b9fa",
      "8bfae4123d41405c940394156a560a73",
      "9ffceb2a10b1470c995aedc27c0d8817",
      "8820bb953ddd49ff810941c9654cfb43",
      "c890d626d09f4ed785a94c34d577c073",
      "3b5da7b236bb48b7ad225fe83de4c94b",
      "868139322d9a4d4fb6e08f181cc959df",
      "ca3840a8e7714c23a3db64e31a66025b",
      "4df6be3072a4480bbacc252f6c4402f0",
      "89ca02cd51ee4cba949713cf55e8e49e",
      "e963ca73a19e426a84dc7f4fd1c37387"
     ]
    },
    "executionInfo": {
     "elapsed": 70109,
     "status": "ok",
     "timestamp": 1757734224778,
     "user": {
      "displayName": "Harish V",
      "userId": "12280508567512914485"
     },
     "user_tz": -330
    },
    "id": "EiD15DioHZJI",
    "outputId": "212c6702-88c7-40a0-d4bb-6076c45d1e72"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2521b6ecb26442818beddd57f2540b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The TextToVideoSDPipeline has been deprecated and will not receive bug fixes or feature updates after Diffusers version 0.33.1. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating video for prompt: A majestic waterfall in the mountains\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ab60bec7c7488fa90d290db839b9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames generated: 16\n",
      "Extended video saved as extended_output_video.mp4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
    "\n",
    "# Load the pipeline\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"damo-vilab/text-to-video-ms-1.7b\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\"\n",
    ")\n",
    "\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "# List of prompts\n",
    "prompts = [\n",
    "    \"A majestic waterfall in the mountains\"\n",
    "]\n",
    "\n",
    "all_frames = []\n",
    "for prompt in prompts:\n",
    "    print(f\"Generating video for prompt: {prompt}\")\n",
    "    result = pipe(prompt, num_inference_steps=30)  # Increase for longer video per prompt\n",
    "    video_frames_batches = result.frames\n",
    "    for batch in video_frames_batches:\n",
    "        for frame in batch:\n",
    "            all_frames.append(frame)\n",
    "\n",
    "print(f\"Total frames generated: {len(all_frames)}\")\n",
    "\n",
    "# Process frames\n",
    "corrected_frames = []\n",
    "for frame in all_frames:\n",
    "    if isinstance(frame, torch.Tensor):\n",
    "        frame = frame.cpu().numpy()\n",
    "    if frame.ndim == 2:\n",
    "        frame = np.stack([frame]*3, axis=-1)\n",
    "    elif frame.ndim == 3 and frame.shape[2] == 1:\n",
    "        frame = np.concatenate([frame]*3, axis=2)\n",
    "    elif frame.ndim == 3 and frame.shape[2] in [3, 4]:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported frame shape: {frame.shape}\")\n",
    "    if frame.dtype != np.uint8:\n",
    "        frame = (frame * 255).clip(0, 255).astype(np.uint8)\n",
    "    corrected_frames.append(frame)\n",
    "\n",
    "# Create video\n",
    "height, width, layers = corrected_frames[0].shape\n",
    "video_filename = 'extended_output_video.mp4'\n",
    "fps = 20\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "video_writer = cv2.VideoWriter(video_filename, fourcc, fps, (width, height))\n",
    "\n",
    "for frame in corrected_frames:\n",
    "    if frame.shape[2] == 4:\n",
    "        frame = frame[:, :, :3]\n",
    "    video_writer.write(frame)\n",
    "\n",
    "video_writer.release()\n",
    "print(f\"Extended video saved as {video_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287,
     "referenced_widgets": [
      "3b3f0305869446158581ffe1673f6f81",
      "817e73ef4cfc441c8f28f4fc629026e6",
      "2515ed716c064a8fb1a7e7aa6720240f",
      "a2e97dc2c17f4f92b6ad23c13ed9ea7c",
      "c7d0f44b26594c8a8a13b99bd72ddba4",
      "3334864d2d564cb0ad43948e4316c517",
      "850bafdc785d41de9e0f11df8fa6f4fa",
      "8755ef57145a4c37b0254b20823f69da",
      "554ebde8aaf046478c98c5cf560269ac",
      "44fbeccc664e45678c5381f8fd566240",
      "c633488361e143dc9f6d2c3d96c9a021",
      "3448610ca36c47c489b4aba42be9a751",
      "588996aeb6dd48c9b93a88b50650efb7",
      "369be7e51a5542049e205cc08c5e5ba4",
      "d7d5c5d42c964137a976d13e608e3fb7",
      "1b46f26e0726479894efd4e136de59e8",
      "71021ad9b3f141c18415805cc22cd67f",
      "36efc533301a46209df003051e1627d2",
      "a4980a5787734bccb261fd5d1da0a5c2",
      "90d79ed86304472dab7e56bc928a4ecc",
      "471ed868570948ae82db027f97e8bd73",
      "a927ca3bcecc4d61b0f494385a6a3bee",
      "86f022811d644d1ca1796d64a3d80c19",
      "2b58390b7acf4d468b403bbd18c3dd61",
      "71c7f69e0aa14e1e96c42459cbfa692b",
      "e2631332784c48309f5bc98590bd5558",
      "7b94a8b0f4574daca71bfbba78dceaeb",
      "90e1810bda69496f9836b5e61baf16da",
      "96edb70935254870b7c88e95cacc6e35",
      "8b21711253a249d9b7ac896f5fa849a3",
      "498556aac7bf4b78974608e1bb79ca19",
      "a6e243ad69384322a1308eb2f525f7d9",
      "6d8c60a9397244c4bf01e9a88c143002",
      "0a0da904eba848219bb887c5a225733c",
      "4ede87ca46c54d1ab1b1481c40e022c4",
      "d37ceb7035a7462db87dbc90a77197df",
      "80478f93f84e4c24b11ccea3e81b89df",
      "6ae45b05bd65422dabbaf3b557a78521",
      "570de45a1f234488a15c6a1b255aa642",
      "da867eb7d0024b0ba194847fc7f5eb99",
      "9c987edb905040deaf99537355e93e9e",
      "e83ee71fa7ce4c0cb2a01da1685ab7d1",
      "2ffeddd9c60b42a2bc17435501b399c6",
      "65aac5b322724f80a0f84d802c2c3f7b"
     ]
    },
    "executionInfo": {
     "elapsed": 120452,
     "status": "ok",
     "timestamp": 1757734543819,
     "user": {
      "displayName": "Harish V",
      "userId": "12280508567512914485"
     },
     "user_tz": -330
    },
    "id": "DwzuaE2JIqK6",
    "outputId": "a348076d-6904-47fa-d564-7a8d437a0784"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3f0305869446158581ffe1673f6f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The TextToVideoSDPipeline has been deprecated and will not receive bug fixes or feature updates after Diffusers version 0.33.1. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating video for prompt: Spiderman is surfing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3448610ca36c47c489b4aba42be9a751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating video for prompt: A spaceship flying over a city\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f022811d644d1ca1796d64a3d80c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating video for prompt: A majestic waterfall in the mountains\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a0da904eba848219bb887c5a225733c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames generated: 48\n",
      "Setting fps = 5 for at least 10 seconds duration\n",
      "Video saved as extended_output_video_10sec.mp4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
    "\n",
    "# Load the pipeline\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"damo-vilab/text-to-video-ms-1.7b\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\"\n",
    ")\n",
    "\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "# List of prompts\n",
    "prompts = [\n",
    "    \"Spiderman is surfing\",\n",
    "    \"A spaceship flying over a city\",\n",
    "    \"A majestic waterfall in the mountains\"\n",
    "]\n",
    "\n",
    "all_frames = []\n",
    "for prompt in prompts:\n",
    "    print(f\"Generating video for prompt: {prompt}\")\n",
    "    result = pipe(prompt, num_inference_steps=40)  # Increase steps to get more frames\n",
    "    video_frames_batches = result.frames\n",
    "    for batch in video_frames_batches:\n",
    "        for frame in batch:\n",
    "            all_frames.append(frame)\n",
    "\n",
    "total_frames = len(all_frames)\n",
    "print(f\"Total frames generated: {total_frames}\")\n",
    "\n",
    "# Decide fps to ensure at least 10 seconds duration\n",
    "target_duration_sec = 10\n",
    "fps = max(5, total_frames // target_duration_sec)  # Ensure at least 10 sec\n",
    "\n",
    "print(f\"Setting fps = {fps} for at least {target_duration_sec} seconds duration\")\n",
    "\n",
    "# Process frames\n",
    "corrected_frames = []\n",
    "for frame in all_frames:\n",
    "    if isinstance(frame, torch.Tensor):\n",
    "        frame = frame.cpu().numpy()\n",
    "    if frame.ndim == 2:\n",
    "        frame = np.stack([frame]*3, axis=-1)\n",
    "    elif frame.ndim == 3 and frame.shape[2] == 1:\n",
    "        frame = np.concatenate([frame]*3, axis=2)\n",
    "    elif frame.ndim == 3 and frame.shape[2] in [3, 4]:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported frame shape: {frame.shape}\")\n",
    "    if frame.dtype != np.uint8:\n",
    "        frame = (frame * 255).clip(0, 255).astype(np.uint8)\n",
    "    corrected_frames.append(frame)\n",
    "\n",
    "# Create video\n",
    "height, width, layers = corrected_frames[0].shape\n",
    "video_filename = 'extended_output_video_10sec.mp4'\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "video_writer = cv2.VideoWriter(video_filename, fourcc, fps, (width, height))\n",
    "\n",
    "for frame in corrected_frames:\n",
    "    if frame.shape[2] == 4:\n",
    "        frame = frame[:, :, :3]\n",
    "    video_writer.write(frame)\n",
    "\n",
    "video_writer.release()\n",
    "print(f\"Video saved as {video_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "42d6b2eaf1c14d71bcc9dfba9a7cfbf0",
      "400ea25dbd8445ee9e18de5f937e068d",
      "c56fba95663e4f988404a588b3410fcc",
      "0df59ecca6ce46e1be79e0b672a29ca3",
      "837bb7e5a7094b668a264200f16ddd23",
      "2c2711f7c1af4896a8e4c8b88f8048c2",
      "782229ba216146e797d0b38cc3b2091b",
      "4997233dfd054c369294bed36276869a",
      "8729e0e23cd24286969db928b1f54085",
      "0fa248c6b3fd4cfeb52140a080ed195c",
      "d615aa766dba413b97b3f6496c4b97ef",
      "8d3aeab69ea14df3a9109ba16e333958",
      "e54a840c9e4c41f9b3f582aaf232e156",
      "c3fca4e9fc9549efa52b6f651f4aa70d",
      "3e3ba07ef51443198e56e59dcbf336a3",
      "d993a36eb33d498ab1b743c30b53857f",
      "b768638cfb2b4f1d85b3edbe3c516a03",
      "da3adcaa60ea434fb55c11620b85b7ea",
      "0ec1a6c232594d03a27253550156aff3",
      "a7f7c045c149432da2a167ad60e84f6d",
      "8ea3ec9d6cda422fa0a5ffbcc85821c9",
      "b1c6100449cd49f8848931101629a5ed",
      "2cb40c8e20cf47d3b77ac3743596b468",
      "44d6513c24704f179b128f6eddebc006",
      "d328187f470747d5a62934d1f23d064c",
      "c90e7477170946e38a0245cf64037cd7",
      "562004d96dc94c7891cd8f60436539ad",
      "be6ccdedc49b423ab71eb6e51e2278c0",
      "6278967d2a1547b0b733fe11ea8992d7",
      "dac74c95b7694b6e9bdfc88fe04984c7",
      "f6fa8a8ea1b94447b238ed6941ad394b",
      "ce32aaf7d6144f34871983ad1142bd37",
      "d1e767da2eae4c9aaa7624c9fa385762",
      "60acbf491b93435ab5609ec9351d973b",
      "c6f42e4b6c054bffb41787b05d64ca7f",
      "4247cdd4d7894c80ae3acbb1dc28976a",
      "446cd2c405a24a3284f68d4ee62cb4f4",
      "684c2cf7c2fc4fc48ac93e5967227bb2",
      "f3cd757b6fb943c8ac801090f949c00e",
      "eaa2032f61f4482693ac21a00a4fbc05",
      "b539faae1de9499c89ef0f5a888100ec",
      "d4e2172f86344507846314145ba849d5",
      "fe335bcf2bbd4a51a022547f5be4b633",
      "913f192e602741ea826cebd906f0da6e",
      "0f96e76b1aeb4c6ea00d0f7e61e02e44",
      "62280f8deb7045c69fbfcc46599b8332",
      "cd1daf22c0a447449106bbce4abeefe2",
      "c92089aacc0047048c51648c75a31ea6",
      "479f17006e0f42cc9cbad8361c0f6e5c",
      "d9f855f7b91e4021ad36abaebada2069",
      "f20fc0c8ed3744648f3455164e12fee9",
      "4e4bcef464d8477d8e05b739ae86eebd",
      "f7e803eb3e3b49989fe9070bd6a26acd",
      "58654f1da99448d8af92cb94ddc562ce",
      "6557e027202742e6b31421f4727a450c",
      "9d195abb57dd4ec7b720abc2c6ca2789",
      "522ebc35a31449d3bc94f381d6394f41",
      "cd56824a57c44b3cb9b7e8b283f17958",
      "618e2d24bc1b4d458179b71d678fcda7",
      "9fa9c381035c4db8b8e503645d1c68dc",
      "e3d5e0a13aa748f6b63a0edacc119039",
      "edbd85a35f1e4c94ad4633b0cfe4cdbe",
      "41d8f31a62794e84860d1c865583b4f2",
      "0d1527b83f1d4efea6e3d1de4bc06c7e",
      "ace1f8c79ad142a4aaa1f508d136b3f2",
      "d6c0c62ed3104f0fb2af82f16ac6ad3b",
      "022ef9ad543345208e0784089a387b4d",
      "af71ae567ac4431ab29f135d380d06d0",
      "a15f6c7466764090bcccf48f580a7d32",
      "6e132a74a9d24de8ae57db17e607f3b2",
      "37a2de57af274509845ad6b9e5e59cd7",
      "05725415c54941d3b6c5482a0db2e31f",
      "0ee2432c1cb6434a98f38d7c4add04ae",
      "2fa25d5956e640ec999cd6a57c41c1d1",
      "7c1c359b8bcf4e89b09137fda55e4887",
      "93dfadbc5df94b59adca1b6fd6ac40eb",
      "375e9b657d4a48548422c86bc3b7e06d",
      "987024e5a5ce4a2fb71b792b8d3411c2",
      "1d12db7d40494f789f1f7225fd98ebc5",
      "667ffbf938b948d787951434e000bac2",
      "cec55af2a02f4b92a7ead092e0073a0a",
      "7d351719b2a34e0794c6b1baa6453205",
      "19976aba37b245a8a08fdfa06990bce2",
      "dbec668b7b434f698bea81bcdc09c499",
      "1b4e945206ba4bad8aed6f0541baf592",
      "f6488082505b4efb80c0d62ec59ae25f",
      "d0f6c082e408408598621cc977734e44",
      "9a731ef12ac54da4a3fca543fa3c60e1",
      "a3e2772a56b842ed8b6b2d6bc1d7a0cd",
      "dc9e59095cf3479bac6fdb31d28f9588",
      "84c7ed5fbed243bc9bb093e93017f96a",
      "8a337d60670645e188fadaf73da03546",
      "97812910ae9d4c1295d3c16ff04d8926",
      "06da35add78142a585b90be3829a2723",
      "0a341cf500e54878958242d75e7d92ce",
      "21df8630df2d462d8b4e83478aabe1fd",
      "acc464579b9e4801a610b3c0ec8787c1",
      "cf191a7e226d49b19fe3c82c80c5b261",
      "04614942f46e442f89cadba4da61739f",
      "c63e37fce64a4887aed9f6729822dff4",
      "79a11cee70cf4b01b2c817a3b51c6ee9",
      "214a10b7a60449468192e2f38d7093f8",
      "a98b7add292441119bf77b6a721dbc8c",
      "f36fb70c3bd247ec891ea1b558d47753",
      "8ede826f478d4ee6ad3ae66c52c9fe54",
      "d9fda338f49848ac9ae6a27b7cb8ff65",
      "01b022d76046413c90e4810d82c9cfd8",
      "101d5774eb264ee59fa19eee4b04a7f3",
      "ddd2ee195ce742caa705ae87ec7f22bb",
      "17e7706c541c4625b16402fe32ba0e9b",
      "ad0188f6ec114a6ba06b8e62de698e2f",
      "ed42e33e6a8947e3a9b7f5914d40dc58",
      "272a280d84d34ce986dec4660f9ff5b6",
      "0a0cf214c0f1491fb18c69d2db23a6cc",
      "1f463a389d124048ade78944b68c6e29",
      "66aa8a532734489dbddcb7c3343d71b4",
      "3a3ab7994570465baa2e044041993f4e",
      "bffc421a809241d7b069d5932de42437",
      "8db6313bb97e470fb7cd1c016fc3e415",
      "8bf3c8ac13314c2588604ead973d309d",
      "311ecfa6ef5044dc80e142814b87440a",
      "3c8cfcd5a3a14c4eb8f7ba45ee09054d",
      "bdd37fe174e34538bf66fdd1f46b2a9a",
      "850b1bc147524cfba1fc69d9626af4a7",
      "d82ae3d9200a4ef798b82d55e6bca5ee",
      "8b1b4c0a4ebc4cd1b0337dd6f6593469",
      "6e54c503fc5948baa7285bd618548de5",
      "7dd40fb6e02741b29e93096fa53d7fa4",
      "b8a54f7ed96e4c38a5e6c8f4bf2e093a",
      "bec4f80894f24fbfb0894165f8c9f2c2",
      "11426f8904914d81b79f09f93d1c1529",
      "b0e94a873d6b4640a69ba8e5b1a71669",
      "648d291754db4c98b46a6ec1a21d7638",
      "0b163259651c4a0abfbf7c5b418cd56c",
      "d7e005634a84433f85317a326f91b85c",
      "072aba7a230942fd95811c050a4a28ec",
      "51f35311ae6443599c26d9fd8ba39cad",
      "6632a19f97a748339dfd1f6b0bef4ac2",
      "70e7f2eeaf8c4626ad967ca2dc17460c",
      "cdb5f83c64d74b59a9a5ee6829aa5135",
      "88391762c023422f86917249150f4e1d",
      "400276de01b0433d8262ce9e28a0d5e1",
      "c6e60f90140946b5806f23cf5f9a402b",
      "775fcbe62a294b8b9b40a023aba587b7",
      "823702429fb146e499eb778131491f01",
      "3b2e78d7e23749249bcf476922b261e7",
      "29924c2ec8bb4800bcd99a58e14c0955",
      "9733a28a13144187af20124f3ba3c300",
      "cce2b983b32d44bba94563bb916cbaa3",
      "b2903f748b80403ba4b0dd57f2c06098",
      "b72a66ffd5d845f38d8b5fba52798a4c",
      "05b696b623564d668adc75bee2f7baad",
      "405eac06e0994568a813565be3f35018",
      "be3c2361591d4b39ac9d5fd7ff72e249"
     ]
    },
    "executionInfo": {
     "elapsed": 382905,
     "status": "ok",
     "timestamp": 1757776070466,
     "user": {
      "displayName": "Harish V",
      "userId": "12280508567512914485"
     },
     "user_tz": -330
    },
    "id": "-6i4OK6TiqhU",
    "outputId": "33523e1c-b977-4902-de12-c54dfaf081ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (0.35.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.0.dev0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers) (8.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from diffusers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from diffusers) (0.34.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from diffusers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.32.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from diffusers) (0.6.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from diffusers) (11.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.34.0->diffusers) (1.1.9)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2025.8.3)\n",
      "\n",
      "Loading model: cerspense/zeroscope_v2_576w\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d6b2eaf1c14d71bcc9dfba9a7cfbf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3aeab69ea14df3a9109ba16e333958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb40c8e20cf47d3b77ac3743596b468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler_config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60acbf491b93435ab5609ec9351d973b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/460 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f96e76b1aeb4c6ea00d0f7e61e02e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/737 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d195abb57dd4ec7b720abc2c6ca2789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022ef9ad543345208e0784089a387b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987024e5a5ce4a2fb71b792b8d3411c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e2772a56b842ed8b6b2d6bc1d7a0cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/pytorch_model.bin:   0%|          | 0.00/681M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63e37fce64a4887aed9f6729822dff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0188f6ec114a6ba06b8e62de698e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unet/diffusion_pytorch_model.bin:   0%|          | 0.00/2.82G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8cfcd5a3a14c4eb8f7ba45ee09054d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vae/diffusion_pytorch_model.bin:   0%|          | 0.00/167M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648d291754db4c98b46a6ec1a21d7638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch /root/.cache/huggingface/hub/models--cerspense--zeroscope_v2_576w/snapshots/6963642a64dbefa93663d1ecebb4ceda2d9ecb28/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /root/.cache/huggingface/hub/models--cerspense--zeroscope_v2_576w/snapshots/6963642a64dbefa93663d1ecebb4ceda2d9ecb28/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch /root/.cache/huggingface/hub/models--cerspense--zeroscope_v2_576w/snapshots/6963642a64dbefa93663d1ecebb4ceda2d9ecb28/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /root/.cache/huggingface/hub/models--cerspense--zeroscope_v2_576w/snapshots/6963642a64dbefa93663d1ecebb4ceda2d9ecb28/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "The TextToVideoSDPipeline has been deprecated and will not receive bug fixes or feature updates after Diffusers version 0.33.1. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating video 1 for prompt: A group of students studying together in a library\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775fcbe62a294b8b9b40a023aba587b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved generated_videos/video_1.mp4\n",
      "\n",
      "ðŸŽ¥ All videos generated and stored in 'generated_videos/' folder\n"
     ]
    }
   ],
   "source": [
    "!pip install diffusers transformers accelerate torch opencv-python\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
    "\n",
    "# Directory to save videos\n",
    "os.makedirs(\"generated_videos\", exist_ok=True)\n",
    "\n",
    "# Choose two different models from Hugging Face\n",
    "models = [\n",
    "    #\"damo-vilab/text-to-video-ms-1.7b\",      # Model 1\n",
    "    \"cerspense/zeroscope_v2_576w\"  # Model 2 (different)\n",
    "]\n",
    "\n",
    "# Prompts related to student/college\n",
    "prompts = [\n",
    "    \"A group of students studying together in a library\",\n",
    "    #\"Students walking across a modern college campus on a sunny day\",\n",
    "    #\"A teacher explaining a concept in a classroom with a smart board\",\n",
    "    #\"College students playing football on the ground\",\n",
    "    #\"A graduation ceremony with students throwing caps in the air\"\n",
    "]\n",
    "\n",
    "# Target video duration\n",
    "target_duration_sec = 10\n",
    "\n",
    "video_counter = 1\n",
    "\n",
    "for model_id in models:\n",
    "    print(f\"\\nLoading model: {model_id}\")\n",
    "    if \"zeroscope\" in model_id:\n",
    "        # zeroscope has no fp16 variant\n",
    "        pipe = DiffusionPipeline.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16   # still okay to run in half precision\n",
    "        )\n",
    "    else:\n",
    "        # models like damo-vilab support fp16 variant\n",
    "        pipe = DiffusionPipeline.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16,\n",
    "            variant=\"fp16\"\n",
    "        )\n",
    "\n",
    "    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "    pipe.enable_model_cpu_offload()\n",
    "\n",
    "\n",
    "    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "    pipe.enable_model_cpu_offload()\n",
    "\n",
    "    # Assign half the prompts to each model\n",
    "    if model_id == models[0]:\n",
    "        model_prompts = prompts[:3]   # First 3 prompts with Model 1\n",
    "    else:\n",
    "        model_prompts = prompts[3:]   # Remaining 2 prompts with Model 2\n",
    "\n",
    "    for prompt in model_prompts:\n",
    "        print(f\"Generating video {video_counter} for prompt: {prompt}\")\n",
    "\n",
    "        result = pipe(prompt, num_inference_steps=40)\n",
    "        video_frames_batches = result.frames\n",
    "\n",
    "        all_frames = []\n",
    "        for batch in video_frames_batches:\n",
    "            for frame in batch:\n",
    "                all_frames.append(frame)\n",
    "\n",
    "        total_frames = len(all_frames)\n",
    "        fps = max(5, total_frames // target_duration_sec)\n",
    "\n",
    "        corrected_frames = []\n",
    "        for frame in all_frames:\n",
    "            if isinstance(frame, torch.Tensor):\n",
    "                frame = frame.cpu().numpy()\n",
    "            if frame.ndim == 2:\n",
    "                frame = np.stack([frame]*3, axis=-1)\n",
    "            elif frame.ndim == 3 and frame.shape[2] == 1:\n",
    "                frame = np.concatenate([frame]*3, axis=2)\n",
    "            elif frame.ndim == 3 and frame.shape[2] in [3, 4]:\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported frame shape: {frame.shape}\")\n",
    "            if frame.dtype != np.uint8:\n",
    "                frame = (frame * 255).clip(0, 255).astype(np.uint8)\n",
    "            corrected_frames.append(frame)\n",
    "\n",
    "        # Save video\n",
    "        height, width, layers = corrected_frames[0].shape\n",
    "        video_filename = f\"generated_videos/video_{video_counter}.mp4\"\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        video_writer = cv2.VideoWriter(video_filename, fourcc, fps, (width, height))\n",
    "\n",
    "        for frame in corrected_frames:\n",
    "            if frame.shape[2] == 4:\n",
    "                frame = frame[:, :, :3]\n",
    "            video_writer.write(frame)\n",
    "\n",
    "        video_writer.release()\n",
    "        print(f\"âœ… Saved {video_filename}\\n\")\n",
    "\n",
    "        video_counter += 1\n",
    "\n",
    "print(\"ðŸŽ¥ All videos generated and stored in 'generated_videos/' folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nwAmvJrxJ0Y"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
    "\n",
    "# Load the pipeline\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"damo-vilab/text-to-video-ms-1.7b\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\"\n",
    ")\n",
    "\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "# List of prompts\n",
    "prompts = [\n",
    "    \"Early school kids playing football during break time\",\n",
    "    \"A person scrolling on a phone in cafe with coffee\",\n",
    "    \"Old lady watering garden grass in the courtyard\",\n",
    "    \"A playful puppy running across a grassy field, chasing butterflies.\",\n",
    "    \"A 3D animation showing how the human heart pumps blood through the body.\"\n",
    "]\n",
    "\n",
    "all_frames = []\n",
    "for prompt in prompts:\n",
    "    print(f\"Generating video for prompt: {prompt}\")\n",
    "    result = pipe(prompt, num_inference_steps=40)  # Increase steps to get more frames\n",
    "    video_frames_batches = result.frames\n",
    "    for batch in video_frames_batches:\n",
    "        for frame in batch:\n",
    "            all_frames.append(frame)\n",
    "\n",
    "total_frames = len(all_frames)\n",
    "print(f\"Total frames generated: {total_frames}\")\n",
    "\n",
    "# Decide fps to ensure at least 10 seconds duration\n",
    "target_duration_sec = 10\n",
    "fps = max(5, total_frames // target_duration_sec)  # Ensure at least 10 sec\n",
    "\n",
    "print(f\"Setting fps = {fps} for at least {target_duration_sec} seconds duration\")\n",
    "\n",
    "# Process frames\n",
    "corrected_frames = []\n",
    "for frame in all_frames:\n",
    "    if isinstance(frame, torch.Tensor):\n",
    "        frame = frame.cpu().numpy()\n",
    "    if frame.ndim == 2:\n",
    "        frame = np.stack([frame]*3, axis=-1)\n",
    "    elif frame.ndim == 3 and frame.shape[2] == 1:\n",
    "        frame = np.concatenate([frame]*3, axis=2)\n",
    "    elif frame.ndim == 3 and frame.shape[2] in [3, 4]:\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported frame shape: {frame.shape}\")\n",
    "    if frame.dtype != np.uint8:\n",
    "        frame = (frame * 255).clip(0, 255).astype(np.uint8)\n",
    "    corrected_frames.append(frame)\n",
    "\n",
    "# Create video\n",
    "height, width, layers = corrected_frames[0].shape\n",
    "video_filename = 'extended_output_video_10sec.mp4'\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "video_writer = cv2.VideoWriter(video_filename, fourcc, fps, (width, height))\n",
    "\n",
    "for frame in corrected_frames:\n",
    "    if frame.shape[2] == 4:\n",
    "        frame = frame[:, :, :3]\n",
    "    video_writer.write(frame)\n",
    "\n",
    "video_writer.release()\n",
    "print(f\"Video saved as {video_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1757829787886,
     "user": {
      "displayName": "Harish V",
      "userId": "12280508567512914485"
     },
     "user_tz": -330
    },
    "id": "1GgRvVU80eFR",
    "outputId": "21843792-e607-42f1-e374-f3222c28fc19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¬ Text-to-Video Generator GUI\n",
      "==================================================\n",
      "Instructions:\n",
      "1. Run: gui = create_video_gui()\n",
      "2. Click 'Load Model' and wait for it to complete\n",
      "3. Enter your prompt(s)\n",
      "4. Adjust settings if needed\n",
      "5. Click generate button\n",
      "6. Download your video when ready!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, Video\n",
    "import os\n",
    "from google.colab import files\n",
    "import threading\n",
    "import time\n",
    "\n",
    "class TextToVideoGUI:\n",
    "    def __init__(self):\n",
    "        self.pipe = None\n",
    "        self.is_model_loaded = False\n",
    "        self.is_generating = False\n",
    "        self.setup_gui()\n",
    "\n",
    "    def setup_gui(self):\n",
    "        \"\"\"Setup the GUI components\"\"\"\n",
    "        # Title\n",
    "        self.title = widgets.HTML(\"<h2>ðŸŽ¬ Text-to-Video Generator</h2>\")\n",
    "\n",
    "        # Model loading section\n",
    "        self.load_button = widgets.Button(\n",
    "            description=\"Load Model\",\n",
    "            button_style='info',\n",
    "            icon='download'\n",
    "        )\n",
    "        self.load_status = widgets.HTML(\"Model not loaded\")\n",
    "\n",
    "        # Prompt input\n",
    "        self.prompt_text = widgets.Textarea(\n",
    "            placeholder=\"Enter your video prompt here...\\nExample: A playful puppy running across a grassy field\",\n",
    "            description=\"Prompt:\",\n",
    "            layout=widgets.Layout(width='100%', height='100px')\n",
    "        )\n",
    "\n",
    "        # Multiple prompts section\n",
    "        self.multi_prompt_text = widgets.Textarea(\n",
    "            placeholder=\"Enter multiple prompts (one per line):\\nEarly school kids playing football\\nA person scrolling on phone in cafe\\nOld lady watering garden\",\n",
    "            description=\"Multiple Prompts:\",\n",
    "            layout=widgets.Layout(width='100%', height='120px')\n",
    "        )\n",
    "\n",
    "        # Settings\n",
    "        self.inference_steps = widgets.IntSlider(\n",
    "            value=25,\n",
    "            min=10,\n",
    "            max=50,\n",
    "            description=\"Inference Steps:\",\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "\n",
    "        self.target_duration = widgets.IntSlider(\n",
    "            value=10,\n",
    "            min=5,\n",
    "            max=30,\n",
    "            description=\"Target Duration (sec):\",\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "\n",
    "        # Generation controls\n",
    "        self.generate_single_btn = widgets.Button(\n",
    "            description=\"Generate Single Video\",\n",
    "            button_style='success',\n",
    "            icon='play',\n",
    "            disabled=True\n",
    "        )\n",
    "\n",
    "        self.generate_multi_btn = widgets.Button(\n",
    "            description=\"Generate Multi-Prompt Video\",\n",
    "            button_style='success',\n",
    "            icon='film',\n",
    "            disabled=True\n",
    "        )\n",
    "\n",
    "        # Progress and status\n",
    "        self.progress = widgets.IntProgress(\n",
    "            value=0,\n",
    "            min=0,\n",
    "            max=100,\n",
    "            description='Progress:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='100%')\n",
    "        )\n",
    "\n",
    "        self.status_text = widgets.HTML(\"Ready to load model\")\n",
    "\n",
    "        # Output section\n",
    "        self.output_area = widgets.Output()\n",
    "\n",
    "        # Download section\n",
    "        self.download_btn = widgets.Button(\n",
    "            description=\"Download Video\",\n",
    "            button_style='warning',\n",
    "            icon='download',\n",
    "            disabled=True\n",
    "        )\n",
    "\n",
    "        self.current_video_path = None\n",
    "\n",
    "        # Bind events\n",
    "        self.load_button.on_click(self.load_model)\n",
    "        self.generate_single_btn.on_click(self.generate_single_video)\n",
    "        self.generate_multi_btn.on_click(self.generate_multi_video)\n",
    "        self.download_btn.on_click(self.download_video)\n",
    "\n",
    "    def display_gui(self):\n",
    "        \"\"\"Display the complete GUI\"\"\"\n",
    "        model_section = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>ðŸ“¥ Model Loading</h3>\"),\n",
    "            widgets.HBox([self.load_button, self.load_status]),\n",
    "        ])\n",
    "\n",
    "        prompt_section = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>âœï¸ Prompts</h3>\"),\n",
    "            self.prompt_text,\n",
    "            self.multi_prompt_text,\n",
    "        ])\n",
    "\n",
    "        settings_section = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>âš™ï¸ Settings</h3>\"),\n",
    "            self.inference_steps,\n",
    "            self.target_duration,\n",
    "        ])\n",
    "\n",
    "        generation_section = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>ðŸŽ¬ Generation</h3>\"),\n",
    "            widgets.HBox([self.generate_single_btn, self.generate_multi_btn]),\n",
    "            self.progress,\n",
    "            self.status_text,\n",
    "        ])\n",
    "\n",
    "        output_section = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>ðŸ“º Output</h3>\"),\n",
    "            self.output_area,\n",
    "            self.download_btn,\n",
    "        ])\n",
    "\n",
    "        main_gui = widgets.VBox([\n",
    "            self.title,\n",
    "            model_section,\n",
    "            prompt_section,\n",
    "            settings_section,\n",
    "            generation_section,\n",
    "            output_section,\n",
    "        ])\n",
    "\n",
    "        display(main_gui)\n",
    "\n",
    "    def load_model(self, b):\n",
    "        \"\"\"Load the diffusion model\"\"\"\n",
    "        if self.is_model_loaded:\n",
    "            self.status_text.value = \"Model already loaded!\"\n",
    "            return\n",
    "\n",
    "        self.load_button.disabled = True\n",
    "        self.load_button.description = \"Loading...\"\n",
    "        self.load_status.value = \"Loading model... This may take a few minutes.\"\n",
    "\n",
    "        try:\n",
    "            # Load the pipeline\n",
    "            self.pipe = DiffusionPipeline.from_pretrained(\n",
    "                \"damo-vilab/text-to-video-ms-1.7b\",\n",
    "                torch_dtype=torch.float16,\n",
    "                variant=\"fp16\"\n",
    "            )\n",
    "\n",
    "            self.pipe.scheduler = DPMSolverMultistepScheduler.from_config(self.pipe.scheduler.config)\n",
    "            self.pipe.enable_model_cpu_offload()\n",
    "\n",
    "            self.is_model_loaded = True\n",
    "            self.load_status.value = \"âœ… Model loaded successfully!\"\n",
    "            self.load_button.description = \"Model Loaded\"\n",
    "            self.load_button.button_style = 'success'\n",
    "\n",
    "            # Enable generation buttons\n",
    "            self.generate_single_btn.disabled = False\n",
    "            self.generate_multi_btn.disabled = False\n",
    "            self.status_text.value = \"Model ready! Enter your prompt and generate video.\"\n",
    "\n",
    "        except Exception as e:\n",
    "            self.load_status.value = f\"âŒ Error loading model: {str(e)}\"\n",
    "            self.load_button.disabled = False\n",
    "            self.load_button.description = \"Load Model\"\n",
    "            self.status_text.value = \"Failed to load model. Please try again.\"\n",
    "\n",
    "    def process_frames(self, all_frames):\n",
    "        \"\"\"Process and correct video frames\"\"\"\n",
    "        corrected_frames = []\n",
    "        for frame in all_frames:\n",
    "            if isinstance(frame, torch.Tensor):\n",
    "                frame = frame.cpu().numpy()\n",
    "            if frame.ndim == 2:\n",
    "                frame = np.stack([frame]*3, axis=-1)\n",
    "            elif frame.ndim == 3 and frame.shape[2] == 1:\n",
    "                frame = np.concatenate([frame]*3, axis=2)\n",
    "            elif frame.ndim == 3 and frame.shape[2] in [3, 4]:\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported frame shape: {frame.shape}\")\n",
    "            if frame.dtype != np.uint8:\n",
    "                frame = (frame * 255).clip(0, 255).astype(np.uint8)\n",
    "            corrected_frames.append(frame)\n",
    "        return corrected_frames\n",
    "\n",
    "    def create_video(self, frames, filename):\n",
    "        \"\"\"Create video from frames\"\"\"\n",
    "        if not frames:\n",
    "            raise ValueError(\"No frames to create video\")\n",
    "\n",
    "        # Calculate FPS for target duration\n",
    "        total_frames = len(frames)\n",
    "        target_duration_sec = self.target_duration.value\n",
    "        fps = max(5, total_frames // target_duration_sec)\n",
    "\n",
    "        height, width = frames[0].shape[:2]\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        video_writer = cv2.VideoWriter(filename, fourcc, fps, (width, height))\n",
    "\n",
    "        for frame in frames:\n",
    "            if frame.shape[2] == 4:\n",
    "                frame = frame[:, :, :3]\n",
    "            # Convert RGB to BGR for OpenCV\n",
    "            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            video_writer.write(frame_bgr)\n",
    "\n",
    "        video_writer.release()\n",
    "        return fps\n",
    "\n",
    "    def generate_single_video(self, b):\n",
    "        \"\"\"Generate video from single prompt\"\"\"\n",
    "        if not self.is_model_loaded:\n",
    "            self.status_text.value = \"Please load the model first!\"\n",
    "            return\n",
    "\n",
    "        prompt = self.prompt_text.value.strip()\n",
    "        if not prompt:\n",
    "            self.status_text.value = \"Please enter a prompt!\"\n",
    "            return\n",
    "\n",
    "        self.run_generation([prompt], \"single_video.mp4\")\n",
    "\n",
    "    def generate_multi_video(self, b):\n",
    "        \"\"\"Generate video from multiple prompts\"\"\"\n",
    "        if not self.is_model_loaded:\n",
    "            self.status_text.value = \"Please load the model first!\"\n",
    "            return\n",
    "\n",
    "        prompts_text = self.multi_prompt_text.value.strip()\n",
    "        if not prompts_text:\n",
    "            self.status_text.value = \"Please enter prompts!\"\n",
    "            return\n",
    "\n",
    "        prompts = [p.strip() for p in prompts_text.split('\\n') if p.strip()]\n",
    "        if not prompts:\n",
    "            self.status_text.value = \"Please enter valid prompts!\"\n",
    "            return\n",
    "\n",
    "        self.run_generation(prompts, \"multi_video.mp4\")\n",
    "\n",
    "    def run_generation(self, prompts, filename):\n",
    "        \"\"\"Run video generation in a separate thread\"\"\"\n",
    "        if self.is_generating:\n",
    "            self.status_text.value = \"Generation already in progress!\"\n",
    "            return\n",
    "\n",
    "        # Disable buttons during generation\n",
    "        self.generate_single_btn.disabled = True\n",
    "        self.generate_multi_btn.disabled = True\n",
    "        self.download_btn.disabled = True\n",
    "        self.is_generating = True\n",
    "\n",
    "        # Clear previous output\n",
    "        with self.output_area:\n",
    "            self.output_area.clear_output()\n",
    "\n",
    "        # Start generation thread\n",
    "        thread = threading.Thread(target=self._generate_video_thread, args=(prompts, filename))\n",
    "        thread.start()\n",
    "\n",
    "    def _generate_video_thread(self, prompts, filename):\n",
    "        \"\"\"Generate video in separate thread\"\"\"\n",
    "        try:\n",
    "            all_frames = []\n",
    "            total_prompts = len(prompts)\n",
    "\n",
    "            for i, prompt in enumerate(prompts):\n",
    "                self.status_text.value = f\"Generating video for prompt {i+1}/{total_prompts}: {prompt[:50]}...\"\n",
    "                self.progress.value = int((i / total_prompts) * 80)  # Up to 80% for generation\n",
    "\n",
    "                result = self.pipe(prompt, num_inference_steps=self.inference_steps.value)\n",
    "                video_frames_batches = result.frames\n",
    "\n",
    "                for batch in video_frames_batches:\n",
    "                    for frame in batch:\n",
    "                        all_frames.append(frame)\n",
    "\n",
    "            self.status_text.value = \"Processing frames and creating video...\"\n",
    "            self.progress.value = 90\n",
    "\n",
    "            # Process frames\n",
    "            corrected_frames = self.process_frames(all_frames)\n",
    "\n",
    "            # Create video\n",
    "            fps = self.create_video(corrected_frames, filename)\n",
    "\n",
    "            self.progress.value = 100\n",
    "            self.status_text.value = f\"âœ… Video generated successfully! ({len(corrected_frames)} frames, {fps} FPS)\"\n",
    "            self.current_video_path = filename\n",
    "\n",
    "            # Display video\n",
    "            with self.output_area:\n",
    "                self.output_area.clear_output()\n",
    "                if os.path.exists(filename):\n",
    "                    print(f\"Video saved as: {filename}\")\n",
    "                    print(f\"Total frames: {len(corrected_frames)}\")\n",
    "                    print(f\"FPS: {fps}\")\n",
    "                    print(f\"Duration: ~{len(corrected_frames)/fps:.1f} seconds\")\n",
    "\n",
    "                    # Try to display video preview\n",
    "                    try:\n",
    "                        display(Video(filename, width=400, height=300))\n",
    "                    except:\n",
    "                        print(\"Video preview not available, but file was created successfully.\")\n",
    "\n",
    "            # Enable download button\n",
    "            self.download_btn.disabled = False\n",
    "\n",
    "        except Exception as e:\n",
    "            self.status_text.value = f\"âŒ Error: {str(e)}\"\n",
    "            with self.output_area:\n",
    "                self.output_area.clear_output()\n",
    "                print(f\"Error during generation: {str(e)}\")\n",
    "\n",
    "        finally:\n",
    "            # Re-enable buttons\n",
    "            self.generate_single_btn.disabled = False\n",
    "            self.generate_multi_btn.disabled = False\n",
    "            self.is_generating = False\n",
    "            self.progress.value = 0\n",
    "\n",
    "    def download_video(self, b):\n",
    "        \"\"\"Download the generated video\"\"\"\n",
    "        if not self.current_video_path or not os.path.exists(self.current_video_path):\n",
    "            self.status_text.value = \"No video to download!\"\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            files.download(self.current_video_path)\n",
    "            self.status_text.value = \"âœ… Video download started!\"\n",
    "        except Exception as e:\n",
    "            self.status_text.value = f\"âŒ Download error: {str(e)}\"\n",
    "\n",
    "# Create and display the GUI\n",
    "def create_video_gui():\n",
    "    \"\"\"Create and display the video generation GUI\"\"\"\n",
    "    gui = TextToVideoGUI()\n",
    "    gui.display_gui()\n",
    "    return gui\n",
    "\n",
    "# Usage instructions\n",
    "print(\"ðŸŽ¬ Text-to-Video Generator GUI\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Instructions:\")\n",
    "print(\"1. Run: gui = create_video_gui()\")\n",
    "print(\"2. Click 'Load Model' and wait for it to complete\")\n",
    "print(\"3. Enter your prompt(s)\")\n",
    "print(\"4. Adjust settings if needed\")\n",
    "print(\"5. Click generate button\")\n",
    "print(\"6. Download your video when ready!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Uncomment the line below to automatically create the GUI\n",
    "# gui = create_video_gui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "2515a37f64404079bfb7297ad5d58aee",
      "ddc92d573277482e932de4928483db34",
      "9befea9562f54e5a9640a523d0b392a9",
      "8c7ee080cc0a428cb8878c7b0b993626",
      "a5c15a4a47fc48eab833ac175e05be32",
      "ae9b9c3d88d7437cbb5870fdc558c20d",
      "5942b833ee204ced9e4f3d521df4c1de",
      "3cbfc838a57a44ebbd22374355651be5",
      "168172dc50bc447280b2305125941c91",
      "420cc6368e8549c0ac133d46c254eeaa",
      "3efdf6bd10f149f2b193be5907d0dafa",
      "7dd1efd04da64e02bcf93a757d7e63fe",
      "10e07c2343f047cb89b8c610412ba4e9",
      "ba9e9631a6af45259c25e10bee5ee5de",
      "afb5b9cb5ba24dd78661a751e3e3892e",
      "ca9e0edd9d984a6faa2cec27f5432158",
      "5e6c9e73035d4f79a591a33eb57e03d2",
      "122ec45c65094701a77d4345ec9160b9",
      "689e7f7cf08e4a06b6e69a510c406c2c",
      "a3167720d6e2425da10c5a56f9e75dcb",
      "e697f996aecf46f199e616f9f48ecc3c",
      "086952d3c28e4ed5919bbe106907e95f",
      "3c8ff9fedb954d3e9d25600b7fa7645e",
      "8a0a367dc84244eda528870c70cfaaa0",
      "bbdd44973f62415b87da549512b09f6f",
      "7879615c06ca4f64b2321358f4cfb030",
      "cf3f949a888844d89547aeaecd460383",
      "2b0d6ff6c0214b639d21b19bbfc367ea",
      "5c3cfa4a92e14a308a2dcd1274b9a78b",
      "ed5e74e63e4e4b85a9c981030df76768",
      "ac756e585f484ffc9bbc34a035e066ba",
      "a4563f6ed6e748f3a6b4ec18631290a8",
      "3c09d12221c040a19aabf8031c430cdf",
      "bdadbc06058442fba9ab737a77e4801c",
      "1dafbcee6c014eb8999e1a4f907372f7",
      "795fafaf9f6643c6a5d0d32abe6b83ac",
      "2f2c27c08c3e4017943c6926b0954934",
      "e5630fe6e0d5421ca8e2ee4475d8a6cd",
      "2fb013cfad5d44fdbf91adac6ee7b906",
      "c8bd29fb76e7427ba5e960d6bde51772",
      "a4254facf1f54ae188d8d0594c95ec3d",
      "75346707669546feb0f25b1f0df3610e",
      "631e308d7c494f7a936f20e31fa74c4e",
      "0f011fe7f37649aea9a11ba88a16c924",
      "592dd9a6484a493f9e1281f0b5b686fa",
      "a5bbfb9e502c4f0e9c982b51dfb62b57",
      "89eb2b7df530410a8ab5cf10c03bc5ee",
      "2c5ba30e199242c894786241c7581b0f",
      "33655391a7d442e3b422604ddb4e8913",
      "2f041ff046954a88b39ed49c97f9b3ad",
      "1430e3e509eb4d75accf1f8293fd14d5",
      "c7540600223c49a4b29d807d8081005f",
      "7948854d06784a62906347b38660cbd6",
      "79c5c1fd015740838677bb68cf063609",
      "4e262093173c4acc81667fbed67e0439",
      "7fc93dc992b143b1bef5b39de03b4856",
      "cd340c041f9e4cb2a88b5a1b44c4cf0e",
      "fe3725c0b9304cbaba0ab63e8334f413",
      "b66cab80b43b4575bffbc1acdd0725fb",
      "438ec5fb25ed463fa360b0c1d233258d",
      "d0b1a59aab8945779945e8175d966c96",
      "30b67033ec9f4d768e47c1c21a1d153b",
      "b9321a26230f4c1c95a2451e44018cc1",
      "d108d04824864a00bf2e81f00f657c65",
      "be4519ade34c43b7935e07563578bbc4",
      "a25ee2323d1b476f816719bd229a2cb6",
      "dde301043ddb44adb19fdd5e59fc9271",
      "0f46777669ef4a42be2c9ab7efed9424",
      "34ff9487382741d1b0f350cb6c74bb1c",
      "271d5b29439f4944912f8cbb298ab71e",
      "6f1d9842593f4429963340a35e112609",
      "0410176ae67440ba86bc922cf25983cd",
      "ee9d6332599a463bb937114d49c65374",
      "b86440d8285d448bab49f9872980184a",
      "52eb8336951e49a1bb59707e36ec0e64",
      "3eb9403cc534456392150b6f6bef182c",
      "c2b0db063a6940daad93c3079b624f20",
      "dafd2695a1924dc6b46ca1bad86f89e4",
      "4e8813ba265d4df6b9f00c32882dc871",
      "74dc5553f9ee46769cb3ce78c031fed8",
      "f3dd678e899c48a58339dae427eb0750",
      "d8b35b9550d94c5dbf583731828c4a75",
      "edbe54d2134544c1a67075603c1f3d17",
      "b29389bc28b1435fa5d0862fd01ac9da",
      "d9600f2f6eba4628ac46a52ab8c35629",
      "71ae41eb5da04cbab1cc1d891de31728",
      "16214bfa89c84d789af02e752af9b019",
      "284819365a0049e0a2f5812a49ac4c9c",
      "22c826e4ec2f4362a5d5ecbbd1708674",
      "5aba1c29a2164fa783a1da998cb07a0a",
      "060b653cd4374b7db17814349c8251df",
      "ac010a9c09fd41d09be75036b4072e97",
      "0624880e23954291847e9f0c571c3099",
      "d01a5b5641fe4431a65470805893d121",
      "0e3d1df5693d4120bcdfc9e09a92ec90",
      "b2da20cf30d145ce99e59c50cbebeab7",
      "99743d3cd4e5481ea9fbf6bd78ce2b3e",
      "f33f74bb45364673b3a9f7a816711acb",
      "036fd837a6204c2587255c50a57b4cd5",
      "10e0c156423b44728171f73726e283d2",
      "60f1da1edc624f0181178807a487360b",
      "58d0226578654a56ae269cb8d7c5a52d",
      "af58d41d73bf41289bdd1fee74d7f93f",
      "fd4ccc696f0a41bebe64b5f5bc5adfe1",
      "511c303845454d999770e07a553971b0",
      "1ccf2e9555e54be4b53b099e06cf5c51",
      "cb5d5288bc784ac38dab93667e5babd6",
      "783a5238c3a84a04aa68f7689a007ec9",
      "32aac086f10a49e4a7074f1a87747499",
      "7428d60cc5e44ebc8f1028dceeed20bd",
      "feff97ae3eb94a7f924f3921505ba784",
      "afa2ded2f0a74483a31e445781b26784",
      "a6fb441e68cf4d73b316dab7eb013229",
      "f1b114e143f640bfbfc1cdacc1cd9188",
      "6366f979a45c4c319916eb6b256229f1",
      "cd799bfd7f4841b990699a58cc81fded",
      "5888420bb11c44718055a80bcfc1f731",
      "f852f6f44aec4d1cb6c8aa390f16f80d",
      "3fd60ee13fb64ec3be14c760cd08e993",
      "ab72f4dd89bb4b1dbd73cb4d42d84d39",
      "d60ff1f97fae42efa7aea1a12fb500b1",
      "95ec1bfc7a1f43008b2acf270ca891e3",
      "dca5bc2bdb8643e0a68802eaf8f6cb1e",
      "db36015d078b4c6a9a0ba9a8c5fd4350",
      "ae8a8e0f42c247c2b8a562eb4bbbeb09",
      "39e0b92cc6164fd89229dfb6444392f4",
      "2b323b2887794bbbb9ef1a7b9357646f",
      "29be5677f08b4e60b1f5ef2f6dfc3c98",
      "3868e1ce517c46f4889a1b16c570c305",
      "eddb7c75cf6f4e658b313ce07d53cbb2",
      "a017ce482e1949108fea11dfb95927bd",
      "4211141fe2fc4e95af506730e62441b7",
      "2a7db24a89704ce381b0026c66699a8e",
      "7b7a976107c34e18924a9aae44cf9710",
      "5aea4f5e676f4f60a181f055b2cf15c5",
      "d41de05efec649f0be17c297074a8501",
      "60034c9deee84cb3ad238e22c32117b0",
      "3b6e8edc21944ef4a2fb28099fcf1fd8",
      "f63a9f107525472ab471b1e7a4561e68",
      "0de1eddd93944a3d8597c42c21cb23f2",
      "10d71452d2db4140af5c27f018c77de6",
      "267ef31ec27d47b0a657826ae3bd75c5",
      "b3916a3f4f664c229a3dab50b6306665",
      "4520fa4075b74d30b80c9bc7bf59ce77",
      "ade83a5b8ef740549cf6667ead09ee9b",
      "bb9ad31913ef442f9e29ff802425fec0",
      "48d9701df5174b479f99da34c6798e8c",
      "f3f956640e944ad6bf90674d946c2e76",
      "6eef81d0d6ac451fa37a8b44d771fd2a",
      "23616e5f373642a79f3e81e973c5562b",
      "7393324075bf4c209d6fd6289c753ecf",
      "97569d5fdd344cba881f5fdd60742611",
      "c1d4d9368c9c40df9944ff643165a5be",
      "477b85ac65954b9c920c465590f57ec8",
      "c2d318f0121d4a3c8fbc8a117f9756b1",
      "1cf41b3a375d4aaf840719f1a4939a0b",
      "d7dd345a78814028b3d74dac056cec5a",
      "a8d81f2cf68541e0847c82ca6483199a",
      "4ad0551d9c41422eab08b83eb0a338bf",
      "7c658403afea49899873511521ebac33",
      "18927f2a679a40c790af738ce0509f88",
      "e178efff986f4f2988b919938f7c8e4f",
      "c3d933a6865b4236ba58f975ba42e8e3",
      "db6bd3129374461a9ecff4f5716e1c19",
      "b21d9e19339d4f549faad23f1b27cb38",
      "e19a22ac827d4f86804807f616a295c4",
      "f35743944f54432783cb304d58eddb06",
      "63bcbb9cddd446d4886cc4ef75069df6",
      "11f1fcd7a63144828ea66d186be4b2a2",
      "9bde5e7db1dc41ccacd646a75c1de9a6",
      "d8e6209d93094ad882f85a7b9346e354",
      "299e91ae625b4b558755a6edacbbe066",
      "ba27484b64ff41759c4ee5fa6c042d6b",
      "25215f5ff7264c70a6312c2763861caa",
      "436aa4bcfea0474db108a71881004c17",
      "7c8536e7006246a6bb56378e08976f41",
      "bb9b683904734df9bfadb137d9348490",
      "c0e675dbaa674277839846798a7b2c46",
      "b6f86229e6b0488292f7d287daa0c959",
      "8b8698056ba14483a729b5c1f20eb315",
      "cabb3fad9cf442ec82e7b1fee6cfaaae",
      "cd73fe64deaa4573a772f99ec23b930d",
      "7d579326f4324d4897fb74718301aede",
      "99855aa5aff341b98615e2b92c129ef6",
      "033c537c56c64cb59bb58ce4c3aecfab",
      "a13bfcd67b584f74a3649bff4d8818bb",
      "bcc60e7f8cea4e9a9bd1154f777e9099",
      "3b450bc440fd493e8efa6ba547b2fe1d",
      "54e9c39d32314f7b84a06a2a072b8d89",
      "788b9523585749d981dfda5e98f8acef",
      "b6216f9f6aff430fb35f5d5786f47c87",
      "3d0258b42c3f4da3bd22ae7146e74862",
      "9756866642664c98a882df4a9c3f7570",
      "b03c59177d2f44fa95ac6a3ddd3b532b",
      "836d72777c1e40b8b9023acfe18b893e",
      "82a964c6b3ff46968d14b642e4e7a1fd",
      "6faa70829b224d00a233a575a91ac036",
      "45a8b5b19f084855a36386595bbadf30",
      "b00d1e48fc724d81b8ee54567f9991ad",
      "f5446025a6804e0dbfe5119b1f7b3b5d",
      "d2d065124bb34ffcbb52b23734929e31",
      "e6f6d831c35d44e0b1a0932f09a40b5a",
      "c47bf913f0a84e67917591dbb07fbba3",
      "f75d24a5c97c4c97b778aa81c3db489a",
      "0b17bc0c730d442d9eb8d27d2e0afa17",
      "463de70bf8c5444da2ef5a823db29135",
      "441b4240997443429e4f024ae209a50a",
      "2961e7a8731a4a63908d947d2aaa7295",
      "a46adf8da1654091b5d173618bbe44f9",
      "0a289f9afce44279b812c4abdd2b52db",
      "4a998854f11145d5b66576740e1b6af3",
      "349b74ff868d420280b4ae5ffec3f830",
      "0d1cef8d45b9473a8fd20cf520c46a3a",
      "587abce10be645bd908a1e258008caae",
      "d3f9b27b6e684058960a8de2aae6f801",
      "5cadba262c474831b8d1a2523f451c69",
      "ce9b54998df44dbaa5d7778a1a586636",
      "361fe0a1751d4a4f930be95ba6cf52df",
      "b2cdb549fb36443f9c1ab4ebf10c0993",
      "51dc3b06ce634f0ba475e812277acfbc",
      "267b69606f8b428d93c2a4f2d28ec6b7",
      "480d5beac82846abaa892edb121f7acc",
      "6171c6e38a344b34aa32f97a5396a4b7"
     ]
    },
    "executionInfo": {
     "elapsed": 120,
     "status": "ok",
     "timestamp": 1757829815208,
     "user": {
      "displayName": "Harish V",
      "userId": "12280508567512914485"
     },
     "user_tz": -330
    },
    "id": "x9ISw7bc0kuG",
    "outputId": "5542134b-af50-4cd0-ec12-1e01a91a9fd4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2515a37f64404079bfb7297ad5d58aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>ðŸŽ¬ Text-to-Video Generator</h2>'), VBox(children=(HTML(value='<h3>ðŸ“¥ Model Loadinâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "271d5b29439f4944912f8cbb298ab71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/384 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3dd678e899c48a58339dae427eb0750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac010a9c09fd41d09be75036b4072e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af58d41d73bf41289bdd1fee74d7f93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler_config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b114e143f640bfbfc1cdacc1cd9188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8a8e0f42c247c2b8a562eb4bbbeb09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d41de05efec649f0be17c297074a8501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/460 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d9701df5174b479f99da34c6798e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/787 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d81f2cf68541e0847c82ca6483199a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/755 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f1fcd7a63144828ea66d186be4b2a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/model.fp16.safetensors:   0%|          | 0.00/681M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8698056ba14483a729b5c1f20eb315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/657 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6216f9f6aff430fb35f5d5786f47c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unet/diffusion_pytorch_model.fp16.safete(â€¦):   0%|          | 0.00/2.82G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f6d831c35d44e0b1a0932f09a40b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vae/diffusion_pytorch_model.fp16.safeten(â€¦):   0%|          | 0.00/167M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1cef8d45b9473a8fd20cf520c46a3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The TextToVideoSDPipeline has been deprecated and will not receive bug fixes or feature updates after Diffusers version 0.33.1. \n"
     ]
    }
   ],
   "source": [
    "gui = create_video_gui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 64,
     "status": "ok",
     "timestamp": 1757830695474,
     "user": {
      "displayName": "Harish V",
      "userId": "12280508567512914485"
     },
     "user_tz": -330
    },
    "id": "K7DHSwUu2DCx",
    "outputId": "af5e7cc2-23e8-4a40-f5c3-a5387348a956"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¬ Web-based Text-to-Video Generator\n",
      "==================================================\n",
      "Setup Instructions for Google Colab:\n",
      "1. Install dependencies:\n",
      "   !pip install gradio diffusers transformers accelerate torch\n",
      "   !pip install opencv-python-headless\n",
      "\n",
      "2. Run the web interface:\n",
      "   demo = launch_web_gui()\n",
      "\n",
      "3. The interface will provide:\n",
      "   - A local URL for Colab\n",
      "   - A public shareable URL\n",
      "   - Full web interface accessible from any device!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
    "import gradio as gr\n",
    "import os\n",
    "import tempfile\n",
    "from typing import List, Optional\n",
    "import threading\n",
    "import time\n",
    "\n",
    "class TextToVideoGenerator:\n",
    "    def __init__(self):\n",
    "        self.pipe = None\n",
    "        self.is_model_loaded = False\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Load the diffusion model\"\"\"\n",
    "        if self.is_model_loaded:\n",
    "            return \"âœ… Model already loaded!\"\n",
    "\n",
    "        try:\n",
    "            # Load the pipeline\n",
    "            self.pipe = DiffusionPipeline.from_pretrained(\n",
    "                \"damo-vilab/text-to-video-ms-1.7b\",\n",
    "                torch_dtype=torch.float16,\n",
    "                variant=\"fp16\"\n",
    "            )\n",
    "\n",
    "            self.pipe.scheduler = DPMSolverMultistepScheduler.from_config(self.pipe.scheduler.config)\n",
    "            self.pipe.enable_model_cpu_offload()\n",
    "\n",
    "            self.is_model_loaded = True\n",
    "            return \"âœ… Model loaded successfully! You can now generate videos.\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"âŒ Error loading model: {str(e)}\"\n",
    "\n",
    "    def process_frames(self, all_frames):\n",
    "        \"\"\"Process and correct video frames\"\"\"\n",
    "        corrected_frames = []\n",
    "        for frame in all_frames:\n",
    "            if isinstance(frame, torch.Tensor):\n",
    "                frame = frame.cpu().numpy()\n",
    "            if frame.ndim == 2:\n",
    "                frame = np.stack([frame]*3, axis=-1)\n",
    "            elif frame.ndim == 3 and frame.shape[2] == 1:\n",
    "                frame = np.concatenate([frame]*3, axis=2)\n",
    "            elif frame.ndim == 3 and frame.shape[2] in [3, 4]:\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported frame shape: {frame.shape}\")\n",
    "            if frame.dtype != np.uint8:\n",
    "                frame = (frame * 255).clip(0, 255).astype(np.uint8)\n",
    "            corrected_frames.append(frame)\n",
    "        return corrected_frames\n",
    "\n",
    "    def create_video(self, frames, target_duration_sec=10):\n",
    "        \"\"\"Create video from frames and return path\"\"\"\n",
    "        if not frames:\n",
    "            raise ValueError(\"No frames to create video\")\n",
    "\n",
    "        # Calculate FPS for target duration\n",
    "        total_frames = len(frames)\n",
    "        fps = max(5, total_frames // target_duration_sec)\n",
    "\n",
    "        height, width = frames[0].shape[:2]\n",
    "\n",
    "        # Create temporary file\n",
    "        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')\n",
    "        filename = temp_file.name\n",
    "        temp_file.close()\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        video_writer = cv2.VideoWriter(filename, fourcc, fps, (width, height))\n",
    "\n",
    "        for frame in frames:\n",
    "            if frame.shape[2] == 4:\n",
    "                frame = frame[:, :, :3]\n",
    "            # Convert RGB to BGR for OpenCV\n",
    "            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            video_writer.write(frame_bgr)\n",
    "\n",
    "        video_writer.release()\n",
    "        return filename, fps, len(frames)\n",
    "\n",
    "    def generate_video(self, prompt: str, inference_steps: int = 25, target_duration: int = 10, progress=gr.Progress()):\n",
    "        \"\"\"Generate video from single prompt\"\"\"\n",
    "        if not self.is_model_loaded:\n",
    "            return None, \"âŒ Please load the model first!\", None\n",
    "\n",
    "        if not prompt.strip():\n",
    "            return None, \"âŒ Please enter a prompt!\", None\n",
    "\n",
    "        try:\n",
    "            progress(0.1, desc=\"Starting generation...\")\n",
    "\n",
    "            # Generate video\n",
    "            progress(0.3, desc=f\"Generating video for: {prompt[:50]}...\")\n",
    "            result = self.pipe(prompt, num_inference_steps=inference_steps)\n",
    "\n",
    "            progress(0.7, desc=\"Processing frames...\")\n",
    "\n",
    "            # Collect all frames\n",
    "            all_frames = []\n",
    "            video_frames_batches = result.frames\n",
    "            for batch in video_frames_batches:\n",
    "                for frame in batch:\n",
    "                    all_frames.append(frame)\n",
    "\n",
    "            # Process frames\n",
    "            corrected_frames = self.process_frames(all_frames)\n",
    "\n",
    "            progress(0.9, desc=\"Creating video file...\")\n",
    "\n",
    "            # Create video\n",
    "            video_path, fps, total_frames = self.create_video(corrected_frames, target_duration)\n",
    "\n",
    "            progress(1.0, desc=\"Complete!\")\n",
    "\n",
    "            status_msg = f\"âœ… Video generated successfully!\\nðŸ“Š Stats: {total_frames} frames, {fps} FPS, ~{total_frames/fps:.1f}s duration\"\n",
    "\n",
    "            return video_path, status_msg, video_path\n",
    "\n",
    "        except Exception as e:\n",
    "            return None, f\"âŒ Error during generation: {str(e)}\", None\n",
    "\n",
    "    def generate_multi_video(self, prompts_text: str, inference_steps: int = 25, target_duration: int = 10, progress=gr.Progress()):\n",
    "        \"\"\"Generate video from multiple prompts\"\"\"\n",
    "        if not self.is_model_loaded:\n",
    "            return None, \"âŒ Please load the model first!\", None\n",
    "\n",
    "        if not prompts_text.strip():\n",
    "            return None, \"âŒ Please enter prompts!\", None\n",
    "\n",
    "        try:\n",
    "            # Parse prompts\n",
    "            prompts = [p.strip() for p in prompts_text.split('\\n') if p.strip()]\n",
    "            if not prompts:\n",
    "                return None, \"âŒ Please enter valid prompts!\", None\n",
    "\n",
    "            progress(0.1, desc=\"Starting multi-prompt generation...\")\n",
    "\n",
    "            all_frames = []\n",
    "            total_prompts = len(prompts)\n",
    "\n",
    "            for i, prompt in enumerate(prompts):\n",
    "                progress_val = 0.1 + (i / total_prompts) * 0.7\n",
    "                progress(progress_val, desc=f\"Generating {i+1}/{total_prompts}: {prompt[:30]}...\")\n",
    "\n",
    "                result = self.pipe(prompt, num_inference_steps=inference_steps)\n",
    "                video_frames_batches = result.frames\n",
    "\n",
    "                for batch in video_frames_batches:\n",
    "                    for frame in batch:\n",
    "                        all_frames.append(frame)\n",
    "\n",
    "            progress(0.8, desc=\"Processing all frames...\")\n",
    "\n",
    "            # Process frames\n",
    "            corrected_frames = self.process_frames(all_frames)\n",
    "\n",
    "            progress(0.95, desc=\"Creating final video...\")\n",
    "\n",
    "            # Create video\n",
    "            video_path, fps, total_frames = self.create_video(corrected_frames, target_duration)\n",
    "\n",
    "            progress(1.0, desc=\"Complete!\")\n",
    "\n",
    "            status_msg = f\"âœ… Multi-prompt video generated!\\nðŸ“Š Stats: {len(prompts)} prompts, {total_frames} frames, {fps} FPS, ~{total_frames/fps:.1f}s duration\"\n",
    "\n",
    "            return video_path, status_msg, video_path\n",
    "\n",
    "        except Exception as e:\n",
    "            return None, f\"âŒ Error during generation: {str(e)}\", None\n",
    "\n",
    "# Initialize the generator\n",
    "generator = TextToVideoGenerator()\n",
    "\n",
    "# Custom CSS for better styling\n",
    "css = \"\"\"\n",
    ".main-header {\n",
    "    color: white;\n",
    "}\n",
    "\n",
    ".section-header {\n",
    "    color: white;\n",
    "}\n",
    "\n",
    ".status-box {\n",
    "    padding: 10px;\n",
    "    border-radius: 5px;\n",
    "    margin: 10px 0;\n",
    "    font-family: monospace;\n",
    "}\n",
    "\n",
    ".success {\n",
    "    background-color: #d4edda;\n",
    "    border: 1px solid #c3e6cb;\n",
    "    color: #155724;\n",
    "}\n",
    "\n",
    ".error {\n",
    "    background-color: #f8d7da;\n",
    "    border: 1px solid #f5c6cb;\n",
    "    color: #721c24;\n",
    "}\n",
    "\n",
    ".video-container {\n",
    "    border: 2px solid #667eea;\n",
    "    border-radius: 10px;\n",
    "    padding: 10px;\n",
    "    margin: 10px 0;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def create_interface():\n",
    "    with gr.Blocks(css=css, title=\"Text-to-Video Generator\") as demo:\n",
    "        gr.HTML(\"\"\"\n",
    "        <div class=\"main-header\">\n",
    "            <h1>Text-to-Video Generator</h1>\n",
    "        </div>\n",
    "        \"\"\")\n",
    "\n",
    "        # Model loading section\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                gr.HTML('<div class=\"section-header\"><h3>ðŸ“¥ Model Management</h3></div>')\n",
    "                load_btn = gr.Button(\"ðŸš€ Load Model\", variant=\"primary\", size=\"lg\")\n",
    "                load_status = gr.Textbox(\n",
    "                    label=\"Model Status\",\n",
    "                    value=\"Model not loaded. Click 'Load Model' to start.\",\n",
    "                    interactive=False\n",
    "                )\n",
    "\n",
    "        load_btn.click(\n",
    "            fn=generator.load_model,\n",
    "            outputs=[load_status]\n",
    "        )\n",
    "\n",
    "        # Main generation interface\n",
    "        with gr.Tabs():\n",
    "            # Single prompt tab\n",
    "            with gr.TabItem(\"ðŸŽ¯ Single Prompt\"):\n",
    "                gr.HTML('<div class=\"section-header\"><h3>Generate from Single Prompt</h3></div>')\n",
    "\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=2):\n",
    "                        single_prompt = gr.Textbox(\n",
    "                            label=\"Enter Your Prompt\",\n",
    "                            placeholder=\"Example: A playful puppy running across a grassy field, chasing butterflies\",\n",
    "                            lines=3\n",
    "                        )\n",
    "\n",
    "                        with gr.Row():\n",
    "                            single_steps = gr.Slider(10, 50, value=25, step=1, label=\"Inference Steps\")\n",
    "                            single_duration = gr.Slider(5, 30, value=10, step=1, label=\"Target Duration (seconds)\")\n",
    "\n",
    "                        single_generate_btn = gr.Button(\"ðŸŽ¬ Generate Video\", variant=\"primary\", size=\"lg\")\n",
    "\n",
    "                    with gr.Column(scale=1):\n",
    "                        single_status = gr.Textbox(\n",
    "                            label=\"Generation Status\",\n",
    "                            value=\"Ready to generate\",\n",
    "                            interactive=False,\n",
    "                            lines=4\n",
    "                        )\n",
    "\n",
    "                with gr.Row():\n",
    "                    single_video_output = gr.Video(label=\"Generated Video\", elem_classes=\"video-container\")\n",
    "                    single_download = gr.File(label=\"Download Video\", visible=False)\n",
    "\n",
    "                single_generate_btn.click(\n",
    "                    fn=generator.generate_video,\n",
    "                    inputs=[single_prompt, single_steps, single_duration],\n",
    "                    outputs=[single_video_output, single_status, single_download]\n",
    "                )\n",
    "\n",
    "            # Multiple prompts tab\n",
    "            with gr.TabItem(\"ðŸŽ­ Multiple Prompts\"):\n",
    "                gr.HTML('<div class=\"section-header\"><h3>Generate Extended Video from Multiple Prompts</h3></div>')\n",
    "\n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=2):\n",
    "                        multi_prompts = gr.Textbox(\n",
    "                            label=\"Enter Multiple Prompts (one per line)\",\n",
    "                            placeholder=\"Early school kids playing football during break time\\nA person scrolling on a phone in cafe with coffee\\nOld lady watering garden grass in the courtyard\\nA playful puppy running across a grassy field\",\n",
    "                            lines=6\n",
    "                        )\n",
    "\n",
    "                        with gr.Row():\n",
    "                            multi_steps = gr.Slider(10, 50, value=25, step=1, label=\"Inference Steps\")\n",
    "                            multi_duration = gr.Slider(5, 30, value=10, step=1, label=\"Target Duration (seconds)\")\n",
    "\n",
    "                        multi_generate_btn = gr.Button(\"ðŸŽ¬ Generate Extended Video\", variant=\"primary\", size=\"lg\")\n",
    "\n",
    "                    with gr.Column(scale=1):\n",
    "                        multi_status = gr.Textbox(\n",
    "                            label=\"Generation Status\",\n",
    "                            value=\"Ready to generate\",\n",
    "                            interactive=False,\n",
    "                            lines=6\n",
    "                        )\n",
    "\n",
    "                with gr.Row():\n",
    "                    multi_video_output = gr.Video(label=\"Generated Video\", elem_classes=\"video-container\")\n",
    "                    multi_download = gr.File(label=\"Download Video\", visible=False)\n",
    "\n",
    "                multi_generate_btn.click(\n",
    "                    fn=generator.generate_multi_video,\n",
    "                    inputs=[multi_prompts, multi_steps, multi_duration],\n",
    "                    outputs=[multi_video_output, multi_status, multi_download]\n",
    "                )\n",
    "\n",
    "\n",
    "    return demo\n",
    "\n",
    "# Function to launch the web interface\n",
    "def launch_web_gui(share=True, debug=False):\n",
    "    \"\"\"\n",
    "    Launch the web-based GUI\n",
    "\n",
    "    Args:\n",
    "        share (bool): If True, creates a public URL that can be shared\n",
    "        debug (bool): If True, enables debug mode\n",
    "    \"\"\"\n",
    "    print(\"ðŸš€ Starting Text-to-Video Web Interface...\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    demo = create_interface()\n",
    "\n",
    "    # Launch with public URL for Colab\n",
    "    demo.launch(\n",
    "        share=share,  # Creates public URL\n",
    "        debug=debug,\n",
    "        height=800,\n",
    "        show_error=True,\n",
    "        quiet=False\n",
    "    )\n",
    "\n",
    "    return demo\n",
    "\n",
    "# Instructions for Colab users\n",
    "print(\"ðŸŽ¬ Web-based Text-to-Video Generator\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Setup Instructions for Google Colab:\")\n",
    "print(\"1. Install dependencies:\")\n",
    "print(\"   !pip install gradio diffusers transformers accelerate torch\")\n",
    "print(\"   !pip install opencv-python-headless\")\n",
    "print(\"\")\n",
    "print(\"2. Run the web interface:\")\n",
    "print(\"   demo = launch_web_gui()\")\n",
    "print(\"\")\n",
    "print(\"3. The interface will provide:\")\n",
    "print(\"   - A local URL for Colab\")\n",
    "print(\"   - A public shareable URL\")\n",
    "print(\"   - Full web interface accessible from any device!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Uncomment to auto-launch (remove the # below)\n",
    "# demo = launch_web_gui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 929
    },
    "executionInfo": {
     "elapsed": 807,
     "status": "ok",
     "timestamp": 1757830702600,
     "user": {
      "displayName": "Harish V",
      "userId": "12280508567512914485"
     },
     "user_tz": -330
    },
    "id": "lw-2auIb2HgP",
    "outputId": "d07fab0f-1f55-4158-b571-793e1b01f5ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Text-to-Video Web Interface...\n",
      "==================================================\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://1d92d6ab397981060f.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1d92d6ab397981060f.gradio.live\" width=\"100%\" height=\"800\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demo = launch_web_gui()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP4PueHw1uJcUCmHyaV/4tm",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
